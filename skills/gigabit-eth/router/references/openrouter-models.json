{
  "data": [
    {
      "id": "minimax/minimax-m2.5",
      "canonical_slug": "minimax/minimax-m2.5-20260211",
      "hugging_face_id": "MiniMaxAI/MiniMax-M2.5",
      "name": "MiniMax: MiniMax M2.5",
      "created": 1770908502,
      "description": "MiniMax-M2.5 is a SOTA large language model designed for real-world productivity. Trained in a diverse range of complex real-world digital working environments, M2.5 builds upon the coding expertise of M2.1 to extend into general office work, reaching fluency in generating and operating Word, Excel, and Powerpoint files, context switching between diverse software environments, and working across different agent and human teams. Scoring 80.2% on SWE-Bench Verified, 51.3% on Multi-SWE-Bench, and 76.3% on BrowseComp, M2.5 is also more token efficient than previous generations, having been trained to optimize its actions and output through planning.",
      "context_length": 196608,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000002",
        "completion": "0.000001"
      },
      "top_provider": {
        "context_length": 196608,
        "max_completion_tokens": 131072,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "z-ai/glm-5",
      "canonical_slug": "z-ai/glm-5-20260211",
      "hugging_face_id": "zai-org/GLM-5",
      "name": "Z.ai: GLM 5",
      "created": 1770829182,
      "description": "GLM-5 is Z.ai’s flagship open-source foundation model engineered for complex systems design and long-horizon agent workflows. Built for expert developers, it delivers production-grade performance on large-scale programming tasks, rivaling leading closed-source models. With advanced agentic planning, deep backend reasoning, and iterative self-correction, GLM-5 moves beyond code generation to full-system construction and autonomous execution.",
      "context_length": 204800,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000075",
        "completion": "0.00000255"
      },
      "top_provider": {
        "context_length": 204800,
        "max_completion_tokens": 131072,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "qwen/qwen3-max-thinking",
      "canonical_slug": "qwen/qwen3-max-thinking-20260123",
      "hugging_face_id": null,
      "name": "Qwen: Qwen3 Max Thinking",
      "created": 1770671901,
      "description": "Qwen3-Max-Thinking is the flagship reasoning model in the Qwen3 series, designed for high-stakes cognitive tasks that require deep, multi-step reasoning. By significantly scaling model capacity and reinforcement learning compute, it delivers major gains in factual accuracy, complex reasoning, instruction following, alignment with human preferences, and agentic behavior.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Qwen",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000012",
        "completion": "0.000006"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "presence_penalty",
        "reasoning",
        "response_format",
        "seed",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openrouter/aurora-alpha",
      "canonical_slug": "openrouter/aurora-alpha",
      "hugging_face_id": "",
      "name": "Aurora Alpha",
      "created": 1770611225,
      "description": "This is a cloaked model provided to the community to gather feedback. A reasoning model designed for speed. It is built for coding assistants, real-time conversational applications, and agentic workflows.\n\nDefault reasoning effort is set to medium for fast responses. For agentic coding use cases, we recommend changing effort to high. \n\nNote: All prompts and completions for this model are logged by the provider and may be used to improve the model.",
      "context_length": 128000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": 50000,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "reasoning_effort",
        "response_format",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "anthropic/claude-opus-4.6",
      "canonical_slug": "anthropic/claude-4.6-opus-20260205",
      "hugging_face_id": "",
      "name": "Anthropic: Claude Opus 4.6",
      "created": 1770219050,
      "description": "Opus 4.6 is Anthropic’s strongest model for coding and long-running professional tasks. It is built for agents that operate across entire workflows rather than single prompts, making it especially effective for large codebases, complex refactors, and multi-step debugging that unfolds over time. The model shows deeper contextual understanding, stronger problem decomposition, and greater reliability on hard engineering tasks than prior generations.\n\nBeyond coding, Opus 4.6 excels at sustained knowledge work. It produces near-production-ready documents, plans, and analyses in a single pass, and maintains coherence across very long outputs and extended sessions. This makes it a strong default for tasks that require persistence, judgment, and follow-through, such as technical design, migration planning, and end-to-end project execution.\n\nFor users upgrading from earlier Opus versions, see our [official migration guide here](https://openrouter.ai/docs/guides/guides/model-migrations/claude-4-6-opus)\n",
      "context_length": 1000000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Claude",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000005",
        "completion": "0.000025",
        "web_search": "0.01",
        "input_cache_read": "0.0000005",
        "input_cache_write": "0.00000625"
      },
      "top_provider": {
        "context_length": 1000000,
        "max_completion_tokens": 128000,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p",
        "verbosity"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "qwen/qwen3-coder-next",
      "canonical_slug": "qwen/qwen3-coder-next-2025-02-03",
      "hugging_face_id": "Qwen/Qwen3-Coder-Next",
      "name": "Qwen: Qwen3 Coder Next",
      "created": 1770164101,
      "description": "Qwen3-Coder-Next is an open-weight causal language model optimized for coding agents and local development workflows. It uses a sparse MoE design with 80B total parameters and only 3B activated per token, delivering performance comparable to models with 10 to 20x higher active compute, which makes it well suited for cost-sensitive, always-on agent deployment.\n\nThe model is trained with a strong agentic focus and performs reliably on long-horizon coding tasks, complex tool usage, and recovery from execution failures. With a native 256k context window, it integrates cleanly into real-world CLI and IDE environments and adapts well to common agent scaffolds used by modern coding tools. The model operates exclusively in non-thinking mode and does not emit <think> blocks, simplifying integration for production coding agents.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Qwen",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000007",
        "completion": "0.0000003",
        "input_cache_read": "0.000000035"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openrouter/free",
      "canonical_slug": "openrouter/free",
      "hugging_face_id": "",
      "name": "Free Models Router",
      "created": 1769917427,
      "description": "The simplest way to get free inference. openrouter/free is a router that selects free models at random from the models available on OpenRouter. The router smartly filters for models that support features needed for your request such as image understanding, tool calling, structured outputs and more. ",
      "context_length": 200000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Router",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "stepfun/step-3.5-flash:free",
      "canonical_slug": "stepfun/step-3.5-flash",
      "hugging_face_id": "stepfun-ai/Step-3.5-Flash",
      "name": "StepFun: Step 3.5 Flash (free)",
      "created": 1769728337,
      "description": "Step 3.5 Flash is StepFun's most capable open-source foundation model. Built on a sparse Mixture of Experts (MoE) architecture, it selectively activates only 11B of its 196B parameters per token. It is a reasoning model that is incredibly speed efficient even at long contexts.",
      "context_length": 256000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 256000,
        "max_completion_tokens": 256000,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "stop",
        "temperature",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "stepfun/step-3.5-flash",
      "canonical_slug": "stepfun/step-3.5-flash",
      "hugging_face_id": "stepfun-ai/Step-3.5-Flash",
      "name": "StepFun: Step 3.5 Flash",
      "created": 1769728337,
      "description": "Step 3.5 Flash is StepFun's most capable open-source foundation model. Built on a sparse Mixture of Experts (MoE) architecture, it selectively activates only 11B of its 196B parameters per token. It is a reasoning model that is incredibly speed efficient even at long contexts.",
      "context_length": 256000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000001",
        "completion": "0.0000003",
        "input_cache_read": "0.00000002"
      },
      "top_provider": {
        "context_length": 256000,
        "max_completion_tokens": 256000,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "stop",
        "temperature",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "arcee-ai/trinity-large-preview:free",
      "canonical_slug": "arcee-ai/trinity-large-preview",
      "hugging_face_id": "arcee-ai/Trinity-Large-Preview",
      "name": "Arcee AI: Trinity Large Preview (free)",
      "created": 1769552670,
      "description": "Trinity-Large-Preview is a frontier-scale open-weight language model from Arcee, built as a 400B-parameter sparse Mixture-of-Experts with 13B active parameters per token using 4-of-256 expert routing. \n\nIt excels in creative writing, storytelling, role-play, chat scenarios, and real-time voice assistance, better than your average reasoning model usually can. But we’re also introducing some of our newer agentic performance. It was trained to navigate well in agent harnesses like OpenCode, Cline, and Kilo Code, and to handle complex toolchains and long, constraint-filled prompts. \n\nThe architecture natively supports very long context windows up to 512k tokens, with the Preview API currently served at 128k context using 8-bit quantization for practical deployment. Trinity-Large-Preview reflects Arcee’s efficiency-first design philosophy, offering a production-oriented frontier model with open weights and permissive licensing suitable for real-world applications and experimentation.",
      "context_length": 131000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 131000,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "response_format",
        "structured_outputs",
        "temperature",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 0.8,
        "top_p": 0.8,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "moonshotai/kimi-k2.5",
      "canonical_slug": "moonshotai/kimi-k2.5-0127",
      "hugging_face_id": "moonshotai/Kimi-K2.5",
      "name": "MoonshotAI: Kimi K2.5",
      "created": 1769487076,
      "description": "Kimi K2.5 is Moonshot AI's native multimodal model, delivering state-of-the-art visual coding capability and a self-directed agent swarm paradigm. Built on Kimi K2 with continued pretraining over approximately 15T mixed visual and text tokens, it delivers strong performance in general reasoning, visual coding, and agentic tool-calling.",
      "context_length": 262144,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000045",
        "completion": "0.0000022",
        "input_cache_read": "0.000000225"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 65535,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "parallel_tool_calls",
        "presence_penalty",
        "reasoning",
        "reasoning_effort",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "upstage/solar-pro-3:free",
      "canonical_slug": "upstage/solar-pro-3",
      "hugging_face_id": "",
      "name": "Upstage: Solar Pro 3 (free)",
      "created": 1769481200,
      "description": "Solar Pro 3 is Upstage's powerful Mixture-of-Experts (MoE) language model. With 102B total parameters and 12B active parameters per forward pass, it delivers exceptional performance while maintaining computational efficiency. Optimized for Korean with English and Japanese support.",
      "context_length": 128000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": "2026-03-02"
    },
    {
      "id": "minimax/minimax-m2-her",
      "canonical_slug": "minimax/minimax-m2-her-20260123",
      "hugging_face_id": "",
      "name": "MiniMax: MiniMax M2-her",
      "created": 1769177239,
      "description": "MiniMax M2-her is a dialogue-first large language model built for immersive roleplay, character-driven chat, and expressive multi-turn conversations. Designed to stay consistent in tone and personality, it supports rich message roles (user_system, group, sample_message_user, sample_message_ai) and can learn from example dialogue to better match the style and pacing of your scenario, making it a strong choice for storytelling, companions, and conversational experiences where natural flow and vivid interaction matter most.",
      "context_length": 65536,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000003",
        "completion": "0.0000012",
        "input_cache_read": "0.00000003"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": 2048,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "writer/palmyra-x5",
      "canonical_slug": "writer/palmyra-x5-20250428",
      "hugging_face_id": "",
      "name": "Writer: Palmyra X5",
      "created": 1769003823,
      "description": "Palmyra X5 is Writer's most advanced model, purpose-built for building and scaling AI agents across the enterprise. It delivers industry-leading speed and efficiency on context windows up to 1 million tokens, powered by a novel transformer architecture and hybrid attention mechanisms. This enables faster inference and expanded memory for processing large volumes of enterprise data, critical for scaling AI agents.",
      "context_length": 1040000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000006",
        "completion": "0.000006"
      },
      "top_provider": {
        "context_length": 1040000,
        "max_completion_tokens": 8192,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "liquid/lfm-2.5-1.2b-thinking:free",
      "canonical_slug": "liquid/lfm-2.5-1.2b-thinking-20260120",
      "hugging_face_id": "LiquidAI/LFM2.5-1.2B-Thinking",
      "name": "LiquidAI: LFM2.5-1.2B-Thinking (free)",
      "created": 1768927527,
      "description": "LFM2.5-1.2B-Thinking is a lightweight reasoning-focused model optimized for agentic tasks, data extraction, and RAG—while still running comfortably on edge devices. It supports long context (up to 32K tokens) and is designed to provide higher-quality “thinking” responses in a small 1.2B model.",
      "context_length": 32768,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 32768,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "liquid/lfm-2.5-1.2b-instruct:free",
      "canonical_slug": "liquid/lfm-2.5-1.2b-instruct-20260120",
      "hugging_face_id": "LiquidAI/LFM2.5-1.2B-Instruct",
      "name": "LiquidAI: LFM2.5-1.2B-Instruct (free)",
      "created": 1768927521,
      "description": "LFM2.5-1.2B-Instruct is a compact, high-performance instruction-tuned model built for fast on-device AI. It delivers strong chat quality in a 1.2B parameter footprint, with efficient edge inference and broad runtime support.",
      "context_length": 32768,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 32768,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-audio",
      "canonical_slug": "openai/gpt-audio",
      "hugging_face_id": "",
      "name": "OpenAI: GPT Audio",
      "created": 1768862569,
      "description": "The gpt-audio model is OpenAI's first generally available audio model. The new snapshot features an upgraded decoder for more natural sounding voices and maintains better voice consistency. Audio is priced at $32 per million input tokens and $64 per million output tokens.",
      "context_length": 128000,
      "architecture": {
        "modality": "text+audio->text+audio",
        "input_modalities": [
          "text",
          "audio"
        ],
        "output_modalities": [
          "text",
          "audio"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000025",
        "completion": "0.00001",
        "audio": "0.000032"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": 16384,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-audio-mini",
      "canonical_slug": "openai/gpt-audio-mini",
      "hugging_face_id": "",
      "name": "OpenAI: GPT Audio Mini",
      "created": 1768859419,
      "description": "A cost-efficient version of GPT Audio. The new snapshot features an upgraded decoder for more natural sounding voices and maintains better voice consistency. Input is priced at $0.60 per million tokens and output is priced at $2.40 per million tokens.",
      "context_length": 128000,
      "architecture": {
        "modality": "text+audio->text+audio",
        "input_modalities": [
          "text",
          "audio"
        ],
        "output_modalities": [
          "text",
          "audio"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000006",
        "completion": "0.0000024",
        "audio": "0.0000006"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": 16384,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "z-ai/glm-4.7-flash",
      "canonical_slug": "z-ai/glm-4.7-flash-20260119",
      "hugging_face_id": "zai-org/GLM-4.7-Flash",
      "name": "Z.ai: GLM 4.7 Flash",
      "created": 1768833913,
      "description": "As a 30B-class SOTA model, GLM-4.7-Flash offers a new option that balances performance and efficiency. It is further optimized for agentic coding use cases, strengthening coding capabilities, long-horizon task planning, and tool collaboration, and has achieved leading performance among open-source models of the same size on several current public benchmark leaderboards.",
      "context_length": 202752,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000006",
        "completion": "0.0000004",
        "input_cache_read": "0.0000000100000002"
      },
      "top_provider": {
        "context_length": 202752,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-5.2-codex",
      "canonical_slug": "openai/gpt-5.2-codex-20260114",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-5.2-Codex",
      "created": 1768409315,
      "description": "GPT-5.2-Codex is an upgraded version of GPT-5.1-Codex optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5.1-Codex, 5.2-Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.",
      "context_length": 400000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000175",
        "completion": "0.000014",
        "web_search": "0.01",
        "input_cache_read": "0.000000175"
      },
      "top_provider": {
        "context_length": 400000,
        "max_completion_tokens": 128000,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "seed",
        "structured_outputs",
        "tool_choice",
        "tools"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "allenai/molmo-2-8b",
      "canonical_slug": "allenai/molmo-2-8b-20260109",
      "hugging_face_id": "allenai/Molmo2-8B",
      "name": "AllenAI: Molmo2 8B",
      "created": 1767996672,
      "description": "Molmo2-8B is an open vision-language model developed by the Allen Institute for AI (Ai2) as part of the Molmo2 family, supporting image, video, and multi-image understanding and grounding. It is based on Qwen3-8B and uses SigLIP 2 as its vision backbone, outperforming other open-weight, open-data models on short videos, counting, and captioning, while remaining competitive on long-video tasks.",
      "context_length": 36864,
      "architecture": {
        "modality": "text+image+video->text",
        "input_modalities": [
          "text",
          "image",
          "video"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000002",
        "completion": "0.0000002"
      },
      "top_provider": {
        "context_length": 36864,
        "max_completion_tokens": 36864,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "max_tokens",
        "presence_penalty",
        "repetition_penalty",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "allenai/olmo-3.1-32b-instruct",
      "canonical_slug": "allenai/olmo-3.1-32b-instruct-20251215",
      "hugging_face_id": "allenai/Olmo-3.1-32B-Instruct",
      "name": "AllenAI: Olmo 3.1 32B Instruct",
      "created": 1767728554,
      "description": "Olmo 3.1 32B Instruct is a large-scale, 32-billion-parameter instruction-tuned language model engineered for high-performance conversational AI, multi-turn dialogue, and practical instruction following. As part of the Olmo 3.1 family, this variant emphasizes responsiveness to complex user directions and robust chat interactions while retaining strong capabilities on reasoning and coding benchmarks. Developed by Ai2 under the Apache 2.0 license, Olmo 3.1 32B Instruct reflects the Olmo initiative’s commitment to openness and transparency.",
      "context_length": 65536,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000002",
        "completion": "0.0000006"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 0.6,
        "top_p": 0.95,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "bytedance-seed/seed-1.6-flash",
      "canonical_slug": "bytedance-seed/seed-1.6-flash-20250625",
      "hugging_face_id": "",
      "name": "ByteDance Seed: Seed 1.6 Flash",
      "created": 1766505011,
      "description": "Seed 1.6 Flash is an ultra-fast multimodal deep thinking model by ByteDance Seed, supporting both text and visual understanding. It features a 256k context window and can generate outputs of up to 16k tokens.",
      "context_length": 262144,
      "architecture": {
        "modality": "text+image+video->text",
        "input_modalities": [
          "image",
          "text",
          "video"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000000075",
        "completion": "0.0000003"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 32768,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "bytedance-seed/seed-1.6",
      "canonical_slug": "bytedance-seed/seed-1.6-20250625",
      "hugging_face_id": "",
      "name": "ByteDance Seed: Seed 1.6",
      "created": 1766504997,
      "description": "Seed 1.6 is a general-purpose model released by the ByteDance Seed team. It incorporates multimodal capabilities and adaptive deep thinking with a 256K context window.",
      "context_length": 262144,
      "architecture": {
        "modality": "text+image+video->text",
        "input_modalities": [
          "image",
          "text",
          "video"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000025",
        "completion": "0.000002"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 32768,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "minimax/minimax-m2.1",
      "canonical_slug": "minimax/minimax-m2.1",
      "hugging_face_id": "MiniMaxAI/MiniMax-M2.1",
      "name": "MiniMax: MiniMax M2.1",
      "created": 1766454997,
      "description": "MiniMax-M2.1 is a lightweight, state-of-the-art large language model optimized for coding, agentic workflows, and modern application development. With only 10 billion activated parameters, it delivers a major jump in real-world capability while maintaining exceptional latency, scalability, and cost efficiency.\n\nCompared to its predecessor, M2.1 delivers cleaner, more concise outputs and faster perceived response times. It shows leading multilingual coding performance across major systems and application languages, achieving 49.4% on Multi-SWE-Bench and 72.5% on SWE-Bench Multilingual, and serves as a versatile agent “brain” for IDEs, coding tools, and general-purpose assistance.\n\nTo avoid degrading this model's performance, MiniMax highly recommends preserving reasoning between turns. Learn more about using reasoning_details to pass back reasoning in our [docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks).",
      "context_length": 196608,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000027",
        "completion": "0.00000095",
        "input_cache_read": "0.0000000299999997"
      },
      "top_provider": {
        "context_length": 196608,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.9,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "z-ai/glm-4.7",
      "canonical_slug": "z-ai/glm-4.7-20251222",
      "hugging_face_id": "zai-org/GLM-4.7",
      "name": "Z.ai: GLM 4.7",
      "created": 1766378014,
      "description": "GLM-4.7 is Z.ai’s latest flagship model, featuring upgrades in two key areas: enhanced programming capabilities and more stable multi-step reasoning/execution. It demonstrates significant improvements in executing complex agent tasks while delivering more natural conversational experiences and superior front-end aesthetics.",
      "context_length": 202752,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000004",
        "completion": "0.0000015",
        "input_cache_read": "0.0000002"
      },
      "top_provider": {
        "context_length": 202752,
        "max_completion_tokens": 65535,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "parallel_tool_calls",
        "presence_penalty",
        "reasoning",
        "reasoning_effort",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      },
      "expiration_date": null
    }
  ]
}